{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472ac31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   phish_id                                          url  \\\n",
      "0   8748799                       https://qrco.de/bfMSpJ   \n",
      "1   7532885                     https://mbarquitecto.cl/   \n",
      "2   9009788    http://allegrolokalnie.oferta-8238064.icu   \n",
      "3   8401021     https://atofredom-105660.weeblysite.com/   \n",
      "4   6677519  http://monirshouvo.github.io/fb_responsive/   \n",
      "\n",
      "                                    phish_detail_url  \\\n",
      "0  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "1  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "2  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "3  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "4  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "\n",
      "             submission_time verified          verification_time online  \\\n",
      "0  2024-09-07T23:21:18+00:00      yes  2024-09-07T23:22:48+00:00    yes   \n",
      "1  2022-06-03T08:19:54+00:00      yes  2022-06-03T17:09:56+00:00    yes   \n",
      "2  2025-03-08T13:04:00+00:00      yes  2025-03-08T13:12:36+00:00    yes   \n",
      "3  2023-12-23T11:29:58+00:00      yes  2023-12-23T11:33:13+00:00    yes   \n",
      "4  2020-07-15T14:02:13+00:00      yes  2020-07-15T14:08:42+00:00    yes   \n",
      "\n",
      "    target  \n",
      "0    Other  \n",
      "1    Other  \n",
      "2  Allegro  \n",
      "3    Other  \n",
      "4    Other  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "url = \"http://data.phishtank.com/data/online-valid.csv.gz\"\n",
    "\n",
    "# Download the compressed file\n",
    "response = requests.get(url)\n",
    "compressed_content = response.content\n",
    "\n",
    "# Decompress the data using gzip\n",
    "decompressed_data = gzip.decompress(compressed_content)\n",
    "\n",
    "# Decode bytes to string\n",
    "csv_string = decompressed_data.decode('utf-8')\n",
    "\n",
    "# Load the CSV data into a DataFrame using StringIO\n",
    "phishing_url = pd.read_csv(StringIO(csv_string))\n",
    "\n",
    "# Optionally shuffle the dataset and reset the index\n",
    "phishing_url = phishing_url.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(phishing_url.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5086f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID        domain\n",
      "0   1    google.com\n",
      "1   2  facebook.com\n",
      "2   3   youtube.com\n",
      "3   4     yahoo.com\n",
      "4   5     baidu.com\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "benign_url = pd.read_csv(\"top-1m.csv\", header=None, names=['ID', 'domain'], nrows=100000)\n",
    "print(benign_url.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f2197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to convert a domain to a full URL\n",
    "def domain_to_url(domain):\n",
    "    # If domain already starts with http, return as is, else prepend \"http://\"\n",
    "    if domain.startswith(\"http\"):\n",
    "        return domain\n",
    "    else:\n",
    "        return \"http://\" + domain + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35cdc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the URL column to \"URL\" if necessary and add Label 1 (phishing)\n",
    "if 'url' in phishing_url.columns:\n",
    "    phishing_url.rename(columns={'url': 'URL'}, inplace=True)\n",
    "else:\n",
    "    raise Exception(\"Phishing dataset does not contain a 'url' column.\")\n",
    "phishing_url['Label'] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67558f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phish_id</th>\n",
       "      <th>URL</th>\n",
       "      <th>phish_detail_url</th>\n",
       "      <th>submission_time</th>\n",
       "      <th>verified</th>\n",
       "      <th>verification_time</th>\n",
       "      <th>online</th>\n",
       "      <th>target</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8748799</td>\n",
       "      <td>https://qrco.de/bfMSpJ</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2024-09-07T23:21:18+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2024-09-07T23:22:48+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7532885</td>\n",
       "      <td>https://mbarquitecto.cl/</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2022-06-03T08:19:54+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2022-06-03T17:09:56+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9009788</td>\n",
       "      <td>http://allegrolokalnie.oferta-8238064.icu</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2025-03-08T13:04:00+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2025-03-08T13:12:36+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Allegro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8401021</td>\n",
       "      <td>https://atofredom-105660.weeblysite.com/</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2023-12-23T11:29:58+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2023-12-23T11:33:13+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6677519</td>\n",
       "      <td>http://monirshouvo.github.io/fb_responsive/</td>\n",
       "      <td>http://www.phishtank.com/phish_detail.php?phis...</td>\n",
       "      <td>2020-07-15T14:02:13+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>2020-07-15T14:08:42+00:00</td>\n",
       "      <td>yes</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phish_id                                          URL  \\\n",
       "0   8748799                       https://qrco.de/bfMSpJ   \n",
       "1   7532885                     https://mbarquitecto.cl/   \n",
       "2   9009788    http://allegrolokalnie.oferta-8238064.icu   \n",
       "3   8401021     https://atofredom-105660.weeblysite.com/   \n",
       "4   6677519  http://monirshouvo.github.io/fb_responsive/   \n",
       "\n",
       "                                    phish_detail_url  \\\n",
       "0  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "1  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "2  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "3  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "4  http://www.phishtank.com/phish_detail.php?phis...   \n",
       "\n",
       "             submission_time verified          verification_time online  \\\n",
       "0  2024-09-07T23:21:18+00:00      yes  2024-09-07T23:22:48+00:00    yes   \n",
       "1  2022-06-03T08:19:54+00:00      yes  2022-06-03T17:09:56+00:00    yes   \n",
       "2  2025-03-08T13:04:00+00:00      yes  2025-03-08T13:12:36+00:00    yes   \n",
       "3  2023-12-23T11:29:58+00:00      yes  2023-12-23T11:33:13+00:00    yes   \n",
       "4  2020-07-15T14:02:13+00:00      yes  2020-07-15T14:08:42+00:00    yes   \n",
       "\n",
       "    target  Label  \n",
       "0    Other      1  \n",
       "1    Other      1  \n",
       "2  Allegro      1  \n",
       "3    Other      1  \n",
       "4    Other      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phishing_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e171cbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://google.com/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://facebook.com/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://youtube.com/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://yahoo.com/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://baidu.com/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    URL  Label\n",
       "0    http://google.com/      0\n",
       "1  http://facebook.com/      0\n",
       "2   http://youtube.com/      0\n",
       "3     http://yahoo.com/      0\n",
       "4     http://baidu.com/      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column \"URL\" with full URLs and assign Label 0 (benign)\n",
    "benign_url['URL'] = benign_url['domain'].apply(domain_to_url)\n",
    "# We only need the URL and Label columns\n",
    "benign_url = benign_url[['URL']].copy()\n",
    "benign_url['Label'] = 0\n",
    "benign_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff15939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2016's top most suspicious TLD and words\n",
    "Suspicious_TLD=['zip','cricket','link','work','party','gq','kim','country','science','tk']\n",
    "Suspicious_Domain=['luckytime.co.kr','mattfoll.eu.interia.pl','trafficholder.com','dl.baixaki.com.br','bembed.redtube.comr','tags.expo9.exponential.com','deepspacer.com','funad.co.kr','trafficconverter.biz']\n",
    "#trend micro's top malicious domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcf1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to count number of dots\n",
    "def countdots(url):\n",
    "    return url.count('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1150442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to count number of delimeters\n",
    "def countdelim(url):\n",
    "    count = 0\n",
    "    delim=[';','_','?','=','&']\n",
    "    for each in url:\n",
    "        if each in delim:\n",
    "            count = count + 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d841f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is IP addr present as th hostname, let's validate\n",
    "\n",
    "import ipaddress as ip #works only in python 3\n",
    "\n",
    "def isip(uri):\n",
    "    try:\n",
    "        if ip.ip_address(uri):\n",
    "            return 1\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e19aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to check the presence of hyphens\n",
    "\n",
    "def isPresentHyphen(url):\n",
    "    return url.count('-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3dab5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to check the presence of @\n",
    "\n",
    "def isPresentAt(url):\n",
    "    return url.count('@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254187c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPresentDSlash(url):\n",
    "    return url.count('//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91a6ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSubDir(url):\n",
    "    return url.count('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19121300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ext(url):\n",
    "    \"\"\"Return the filename extension from url, or ''.\"\"\"\n",
    "\n",
    "    root, ext = splitext(url)\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7820eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSubDomain(subdomain):\n",
    "    if not subdomain:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(subdomain.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a604ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countQueries(query):\n",
    "    if not query:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(query.split('&'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebda091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "featureSet = pd.DataFrame(columns=('url','no of dots','presence of hyphen','len of url','presence of at',\\\n",
    "'presence of double slash','no of subdir','no of subdomain','len of domain','no of queries','is IP','presence of Suspicious_TLD',\\\n",
    "'presence of suspicious domain','create_age(months)','expiry_age(months)','update_age(days)','country','file extension','label'))'''\n",
    "\n",
    "featureSet = pd.DataFrame(columns=('url','no of dots','presence of hyphen','len of url','presence of at',\\\n",
    "'presence of double slash','no of subdir','no of subdomain','len of domain','no of queries','is IP','presence of Suspicious_TLD',\\\n",
    "'presence of suspicious domain','label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac370982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in ./.venv/lib/python3.10/site-packages (5.3.0)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from tldextract) (3.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in ./.venv/lib/python3.10/site-packages (from tldextract) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in ./.venv/lib/python3.10/site-packages (from tldextract) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in ./.venv/lib/python3.10/site-packages (from tldextract) (3.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.1.0->tldextract) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.1.0->tldextract) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d71c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (2.2.5)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dba18f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "\n",
    "def getFeatures(url, label):\n",
    "    result = []\n",
    "    url = str(url)\n",
    "\n",
    "    #add the url to feature set\n",
    "    result.append(url)\n",
    "\n",
    "    #parse the URL and extract the domain information\n",
    "    path = urlparse(url)\n",
    "    ext = tldextract.extract(url)\n",
    "\n",
    "    #counting number of dots in subdomain\n",
    "    result.append(countdots(ext.subdomain))\n",
    "\n",
    "    #checking hyphen in domain\n",
    "    result.append(isPresentHyphen(path.netloc))\n",
    "\n",
    "    #length of URL\n",
    "    result.append(len(url))\n",
    "\n",
    "    #checking @ in the url\n",
    "    result.append(isPresentAt(path.netloc))\n",
    "\n",
    "    #checking presence of double slash\n",
    "    result.append(isPresentDSlash(path.path))\n",
    "\n",
    "    #Count number of subdir\n",
    "    result.append(countSubDir(path.path))\n",
    "\n",
    "    #number of sub domain\n",
    "    result.append(countSubDomain(ext.subdomain))\n",
    "\n",
    "    #length of domain name\n",
    "    result.append(len(path.netloc))\n",
    "\n",
    "    #count number of queries\n",
    "    result.append(len(path.query))\n",
    "\n",
    "    #Adding domain information\n",
    "\n",
    "    #if IP address is being used as a URL\n",
    "    result.append(isip(ext.domain))\n",
    "\n",
    "    #presence of Suspicious_TLD\n",
    "    result.append(1 if ext.suffix in Suspicious_TLD else 0)\n",
    "\n",
    "    #presence of suspicious domain\n",
    "    # Use attributes instead of indexing\n",
    "    result.append(1 if '.'.join([ext.domain, ext.suffix]) in Suspicious_Domain else 0 )\n",
    "\n",
    "    '''\n",
    "\n",
    "    #Get domain information by asking whois\n",
    "    domain = '.'.join(ext[1:]) # This line was likely the problem in your original code\n",
    "    '''\n",
    "\n",
    "    #result.append(get_ext(path.path))\n",
    "    result.append(str(label))\n",
    "    return result\n",
    "\n",
    "    #Yay! finally done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dac565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine Datasets ---\n",
    "combined_df = pd.concat([benign_url, phishing_url[['URL', 'Label']]], ignore_index=True, )\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b16653e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.google.com/presentation/d/e/2PACX...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ezpass.com-gltu.xin/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://ladbrokes.com/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://ipsimisul.com/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://gruks.com/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  Label\n",
       "0  https://docs.google.com/presentation/d/e/2PACX...      1\n",
       "1                       https://ezpass.com-gltu.xin/      1\n",
       "2                              http://ladbrokes.com/      0\n",
       "3                              http://ipsimisul.com/      0\n",
       "4                                  http://gruks.com/      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d7f63cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e51c60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/160940 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160940/160940 [11:00<00:00, 243.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(combined_df))):\n",
    "    features = getFeatures(combined_df[\"URL\"].loc[i], combined_df[\"Label\"].loc[i])\n",
    "    featureSet.loc[i] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9800c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_col_names = [\n",
    "    'url',\n",
    "    'no of dots',\n",
    "    'presence of hyphen',\n",
    "    'len of url',\n",
    "    'presence of at',\n",
    "    'presence of double slash',\n",
    "    'no of subdir',\n",
    "    'no of subdomain',\n",
    "    'len of domain',\n",
    "    'no of queries',\n",
    "    'is IP',\n",
    "    'presence of Suspicious_TLD',\n",
    "    'presence of suspicious domain',\n",
    "    'label'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00bf6359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of columns matches number of names. Assigning new column names...\n",
      "\n",
      "--- Updated featureSet columns ---\n",
      "['url', 'no of dots', 'presence of hyphen', 'len of url', 'presence of at', 'presence of double slash', 'no of subdir', 'no of subdomain', 'len of domain', 'no of queries', 'is IP', 'presence of Suspicious_TLD', 'presence of suspicious domain', 'label']\n",
      "\n",
      "--- Updated featureSet head ---\n",
      "                               url  no of dots  presence of hyphen  \\\n",
      "0  http://auxilioplandesocios.com/           0                   0   \n",
      "1       http://e-solutionsinc.com/           0                   1   \n",
      "2    http://illustrations.free.fr/           0                   0   \n",
      "3               http://tomari.org/           0                   0   \n",
      "4           http://firefox.net.cn/           0                   0   \n",
      "\n",
      "   len of url  presence of at  presence of double slash  no of subdir  \\\n",
      "0          31               0                         0             1   \n",
      "1          26               0                         0             1   \n",
      "2          29               0                         0             1   \n",
      "3          18               0                         0             1   \n",
      "4          22               0                         0             1   \n",
      "\n",
      "   no of subdomain  len of domain  no of queries  is IP  \\\n",
      "0                0             23              0      0   \n",
      "1                0             18              0      0   \n",
      "2                1             21              0      0   \n",
      "3                0             10              0      0   \n",
      "4                0             14              0      0   \n",
      "\n",
      "   presence of Suspicious_TLD  presence of suspicious domain label  \n",
      "0                           0                              0     0  \n",
      "1                           0                              0     0  \n",
      "2                           0                              0     0  \n",
      "3                           0                              0     0  \n",
      "4                           0                              0     0  \n"
     ]
    }
   ],
   "source": [
    "# Check if the number of columns matches the number of names provided\n",
    "if len(featureSet.columns) == len(correct_col_names):\n",
    "    print(\"\\nNumber of columns matches number of names. Assigning new column names...\")\n",
    "    # Assign the list of correct names to the DataFrame's columns attribute\n",
    "    featureSet.columns = correct_col_names\n",
    "\n",
    "    print(\"\\n--- Updated featureSet columns ---\")\n",
    "    print(featureSet.columns.tolist()) # Use tolist() for cleaner output\n",
    "\n",
    "    print(\"\\n--- Updated featureSet head ---\")\n",
    "    print(featureSet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e549adb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    100000\n",
      "1     60940\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now group by 'label' and display the counts\n",
    "print(featureSet.groupby('label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7139f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9beed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "337a4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = featureSet.drop(['url','label'],axis=1).values\n",
    "y = featureSet['label'].values\n",
    "\n",
    "model = { \"LogisticRegression\":LogisticRegression() }\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28d6efac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : 0.9757362992419535 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhanh/Donapi/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for algo in model:\n",
    "    clf = model[algo]\n",
    "    clf.fit(X_train,y_train)\n",
    "    score = clf.score(X_test,y_test)\n",
    "    print (\"%s : %s \" %(algo, score))\n",
    "    results[algo] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7f3a7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "574c5d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhanh/Donapi/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logistic_model.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(clf, 'logistic_model.pkl')  # You can choose any filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5d4431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : 0.9757362992419535 \n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "loaded_model = joblib.load('logistic_model.pkl')\n",
    "score = clf.score(X_test,y_test)\n",
    "print (\"%s : %s \" %(algo, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f741f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate : 1.220000 %\n",
      "False negative rate : 4.522481 %\n"
     ]
    }
   ],
   "source": [
    "clf = model[winner]\n",
    "res = clf.predict(X)\n",
    "mt = confusion_matrix(y, res)\n",
    "print(\"False positive rate : %f %%\" % ((mt[0][1] / float(sum(mt[0])))*100))\n",
    "print('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db865722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://github.com/bahmutov/all-paths/issues']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def extract_urls(text):\n",
    "    \"\"\"Extracts potential URLs from a string using regex.\"\"\"\n",
    "    # This regex is relatively simple and might need refinement for edge cases\n",
    "    # It looks for http://, https://, or ftp:// followed by non-space characters\n",
    "    url_pattern = re.compile(r'(?:https?|ftp)://[^\\s/$.?#].[^\\s]*', re.IGNORECASE)\n",
    "    return url_pattern.findall(text)\n",
    "\n",
    "print(extract_urls(\"\"\" {\n",
    "  \"name\": \"@bahmutov/all-paths\",\n",
    "  \"description\": \"Given an object returns list of all possible paths to its properties\",\n",
    "  \"version\": \"1.0.2\",\n",
    "  \"author\": \"Gleb Bahmutov <gleb.bahmutov@gmail.com>\",\n",
    "  \"bugs\": \"https://github.com/bahmutov/all-paths/issues\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8d341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
